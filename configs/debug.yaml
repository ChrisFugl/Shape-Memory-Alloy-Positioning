batch_size: 256
environment:
  type: debug
  goal_position: 5.0
  goal_tolerance: 3.0
  min_start_position: 1.0
  max_start_position: 2.0
  pass_scale_interval_to_policy: true
  scale_action: false
exploration_steps: 256
evaluation_steps: 64
gradient_steps: 50
iterations: 1000
max_buffer_size: 100000
max_trajectory_length: 10
min_num_steps_before_training: 0
model:
  discount_factor: 0.99
  exponential_weight: 0.005
  learning_rate_policy: 0.0003
  learning_rate_q: 0.0003
  network:
    hidden_size: 32
    number_of_hidden_layers: 2
  reward_scale: 3.0
  target_update_period: 1
  use_automatic_entropy_tuning: true
policy:
  type: tanh_gaussian
  network:
    hidden_size: 32
    number_of_hidden_layers: 2
save_model: saved_models/debug/