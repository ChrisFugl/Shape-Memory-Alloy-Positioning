batch_size: 256
collect_actions: true
collect_actions_every: 200
exploration_steps: 128
evaluation_steps: 32
gradient_steps: 128
iterations: -1
max_buffer_size: 1000000
max_trajectory_length: 15
min_num_steps_before_training: 0
save_checkpoint_interval_s: 1800
save_model: saved_models/real_time/
environment:
  type: real_time
  host: 143.248.253.145
  port_read: 6340
  port_write: 6340
  bytes_per_value: 9
  next_state_wait_time: 0.2
  action_decimal_precision: 4
  values_per_observation: 10
  action_digit_precision: 3
  goal_position: 0.05
  goal_tolerance: 0.01
  goal_time_tolerance_s: 2
  scale_action: True
  pass_scale_interval_to_policy: True
  max_position: 0.5
  max_exponential_threshold: 0.4
  max_voltage: 12.0
model:
  discount_factor: 0.99
  exponential_weight: 0.005
  learning_rate_policy: 0.0003
  learning_rate_q: 0.0003
  network:
    hidden_size: 32
    number_of_hidden_layers: 2
  reward_scale: 3.0
  target_update_period: 1
  use_automatic_entropy_tuning: true
policy:
  type: tanh_gaussian
  network:
    hidden_size: 32
    number_of_hidden_layers: 2
